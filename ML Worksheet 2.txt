Ans 12- The bias–variance tradeoff is the property of a set of predictive models whereby models with a lower bias in parameter estimation have a higher variance of the parameter estimates across sample.

Ans-11-A learning rate that is too large will result in weight updates that will be too large and the performance of the model (such as its loss on the training dataset) will oscillate over training epochs. Oscillating performance is said to be caused by weights that diverge (are divergent).

Ans 9- The procedure has a single parameter called k that refers to the number of groups that a given data sample is to be split into. As such, the procedure is often called k-fold cross-validation. When a specific value for k is chosen, it may be used in place of k in the reference to the model, such as k=10 becoming 10-fold cross-validation.

Ans- 10- It is the process of  optimize the  model architecture  for a given model,due to this we will able to explore a range of possibilities. In true machine learning fashion, we'll ideally ask the machine to perform this exploration and select the optimal model architecture automatically. Parameters which define the model architecture are referred to as hyperparameters and thus this process of searching for the ideal model architecture is referred to as hyperparameter tuning.

Ans-13-Regularisation is a technique used to reduce the errors by fitting the function appropriately on the given training set and avoid overfitting.

Ans-14-Gradient boosting generates learners during the learning process. It build first learner to predict the values/labels of samples, and calculate the loss (the difference between the outcome of the first learner and the real value). It will build a second learner to predict the loss after the first step. The step continues to learn the third, forth… until certain threshold.

Adaboost requires users specify a set of weak learners (alternatively, it will randomly generate a set of weak learner before the real learning process). It will learn the weights of how to add these learners to be a strong learner.

ans-8-Out-of-bag (OOB) error, also called out-of-bag estimate, is a method of measuring the prediction error of random forests, boosted decision trees, and other machine learning models utilizing bootstrap aggregating (bagging) to sub-sample data samples used for training.

Ans-6-Ensemble is a Machine Learning concept in which the idea is to train multiple models using the same learning algorithm. The ensembles take part in a bigger group of methods, called multiclassifiers, where a set of hundreds or thousands of learners with a common objective are fused together to solve the problem.

Ans-7        Bagging techniques                                                               Boosting techniques
        
        1 Simplest way of combining predictions that                                    1 A way of combining predictions that
          belong to the same type.                                                        belong to the different types
        2-  Aim to decrease variance, not bias.                                         2- Aim to decrease bias, not variance.
        3- Each model receives equal weight                                             3-Models are weighted according to their performance.

Ans 4-Gini index or Gini impurity measures the degree or probability of a particular variable being wrongly classified when it is randomly chosen. 


Ans-3-TSS - 
In statistical data analysis the total sum of squares (TSS or SST) is a quantity that appears as part of a standard way of presenting results of such analyses.

ESS- 
The explained sum of squares (ESS) is the sum of the squares of the deviations of the predicted values from the mean value of a response variable, in a standard regression model 

RSS- 
A residual sum of squares (RSS) is a statistical technique used to measure the amount of variance in a data set that is not explained by a regression model.

Total SS = Explained SS + Residual Sum of Squares.


Ans-2-R2 squrare represents the proportion of the variance in your data which is explained by your model; the closer to one, the better the fit.

Ans 1- The polynomial kernel is a kernel function commonly used with support vector machines (SVMs) and other kernelized models, that represents the similarity of vectors (training samples) in a feature space over polynomials of the original variables, allowing learning of non-linear models.

RBF: z=exp(-γ||χ-y ||2)
Linear: z=y*x+b
polynominal: z=(y*x+b)2